
- 1 Layer 1 Development
  - 1.1 Layer 1 Development: Main Components
 
  | Component                             | What You Do / Configure                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Purpose & Notes                                                                                                                                                                               |
  | ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | **1. Microsoft Purview**              | - **Register your data sources** (e.g., ADLS, SQL) to enable scanning <br> - Configure **scheduled scans** to discover schema and classify data <br> - Define **business glossary terms** (optional)                                                                                                                                                                                                                                                                                       | - Purview **does not validate incoming data** <br> - It **catalogs** schemas after data lands in raw layer or elsewhere <br> - Helps with data governance, classification, and lineage        |
  | **2. JSON Schema Inclusion**          | - Create a **JSON Schema file** (e.g., `schema.json`) with your rules: types, required fields, formats, min/max, etc. <br> - Store this schema file in a **secure accessible location** (Azure Blob Storage preferred) <br> - Keep the schema version-controlled and updated                                                                                                                                                                                                               | - This schema is your **source of truth** for data validation <br> - Enables strict validation beyond ADF’s built-in capabilities (like `"format": "email"`)                                  |
  | **3. Azure Data Factory (ADF) Setup** | - Define a **dataset schema** in ADF dataset definitions with basic types and required fields (simple validation) <br> - Build **data flows** or pipelines with expression validations (minLength, regex) for additional runtime checks <br> - Integrate **Azure Function activity** that fetches the JSON schema file and runs full validation (using JSONSchema lib) on incoming data <br> - Handle validation results (pass/fail), route invalid records (quarantine), and raise alerts | - ADF dataset schema supports basic validation only <br> - Complex validations are offloaded to Azure Functions or custom services <br> - ADF orchestrates ingestion and validation workflows |


  - 1.2 Layer 1 Monitoring & Alerting
  
  | Step | Description                                                   | Services / Tools Used                                                                                                                                               | What It Does                                                                                                                                                                 | How It Gets Configured                                                                                                                                                                                 |
  | ---- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
  | 1    | Enable **diagnostic logging** in Azure Data Factory pipelines | - **Azure Data Factory (ADF)** <br> - **Azure Portal** (Monitor & Manage section) <br> - **Diagnostic Settings** in Azure Monitor                                   | ADF runs pipelines and generates logs about pipeline runs and activities. Diagnostic Settings enable exporting these logs.                                                   | In Azure Portal, go to your ADF resource → **Monitor & Manage** → **Diagnostic Settings** → Enable and configure to send logs to Log Analytics or Storage                                              |
  | 2    | Implement detailed logging inside **Azure Functions**         | - **Azure Functions** <br> - **Application Insights** (optional, for enhanced logging and telemetry)                                                                | Azure Functions execute your custom code. Logging inside the function tracks errors, validation results, and processing steps. Application Insights captures telemetry data. | Add logging statements (`logging.info()`, `logging.error()`) in your function code. Enable Application Insights integration from Azure Functions settings for richer logs and metrics.                 |
  | 3    | Send logs to **Azure Monitor / Log Analytics workspace**      | - **Azure Data Factory** (configured to send logs) <br> - **Azure Functions** (configured to send logs) <br> - **Azure Monitor** <br> - **Log Analytics workspace** | Azure Monitor centralizes monitoring data. Log Analytics workspace stores and allows querying logs from various sources. Logs from ADF and Functions are pushed here.        | In Diagnostic Settings of ADF and Functions, specify sending logs to Log Analytics workspace. In Azure Monitor, configure the workspace and data retention policies.                                   |
  | 4    | Create **alerts and dashboards**                              | - **Azure Monitor Alerts** (for email, SMS, webhook notifications) <br> - **Power BI** (for dashboards and visualization)                                           | Azure Monitor Alerts trigger notifications based on defined log query conditions. Power BI visualizes data in dashboards for monitoring health.                              | Define alert rules in Azure Monitor using Kusto Query Language (KQL) queries on logs. Configure notification channels (email, SMS). Use Power BI to connect to Log Analytics or exported data sources. |


  - 1.3 Other Optional Layer 1 Considerations
  
  | Task                 | Description                                                                                          |
  | -------------------- | ---------------------------------------------------------------------------------------------------- |
  | **Error Handling**   | Design quarantine or dead-letter storage for invalid data <br> (e.g., blob container `/quarantine/`) |
  | **Schema Evolution** | Plan how to handle schema changes over time (versioning schemas, backward compatibility)             |
  | **Security**         | Secure access to JSON Schema files and data sources using Managed Identities and RBAC                |


- 2 Layer 2 Development
  - 2.1 Layer 2 Main Parts
  | Step | Description                 | Services / Tools Used                                    | What It Does                                                                                 | How It Gets Configured                                                                                                   |
  | ---- | --------------------------- | -------------------------------------------------------- | -------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
  | 1    | Ingest Raw Data             | Azure Data Factory (ADF), Azure Blob Storage / ADLS Gen2 | Reads raw JSON data from storage into the data flow for validation                           | Create an ADF dataset pointing to raw data location. Use it as Source in Mapping Data Flow with schema defined/imported. |
  | 2    | Perform Runtime Validations | ADF Mapping Data Flow                                    | Applies runtime business logic validations such as minLength, regex, enums, and value ranges | Use Derived Column transformations with expressions like `length()`, `matches()`, `in` for field validation.             |
  | 3    | Create Validation Flags     | ADF Mapping Data Flow                                    | Adds columns indicating whether each row passes validation and details of any errors         | Add boolean flags (`isValidRecord`) and error message columns (`errorReason`) in Derived Column transformations.         |
  | 4    | Conditional Split           | ADF Mapping Data Flow                                    | Splits data stream into valid and invalid records based on validation flags                  | Use Conditional Split transformation to route valid and invalid data streams using the validation flags.                 |
  | 7    | Integration with Layer 3    | ADF Pipelines, Azure Functions                           | Sends valid data for complex validations like `"format": "email"` and advanced schema checks | Use Azure Function activity in ADF pipeline to call external validation service. Route data based on function response.  |
  | 8    | Testing & Deployment        | Azure Data Factory UI, Azure DevOps / GitHub             | Ensures data flows handle all validation cases and deploys them into production environments | Test data flows with different cases; use Git/Azure DevOps for version control and CI/CD deployment pipelines.           |

  - 2.2 Layer 2 Optional/ Additional Parts

  | Step | Description                 | Services / Tools Used                                       | What It Does                                                                                 | How It Gets Configured                                                                                                |
  | ---- | --------------------------- | ----------------------------------------------------------- | -------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |
  | 5    | Quarantine Invalid Data     | Azure Blob Storage / ADLS Gen2, ADF Mapping Data Flow       | Stores invalid data records separately for auditing and troubleshooting                      | Sink invalid output to quarantine storage path (e.g., `/quarantine/yyyy/MM/dd/`) in Data Flow Sink transformation.    |
  | 6    | Enable Logging & Monitoring | Azure Data Factory Monitoring, Azure Monitor, Log Analytics | Tracks pipeline and data flow execution metrics, validation failures, and operational health | Enable diagnostic settings, configure logs to Azure Monitor/Log Analytics; set alerts and monitor runs in ADF portal. |


